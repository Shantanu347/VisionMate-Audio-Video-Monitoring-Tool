{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b3aaa1",
   "metadata": {},
   "source": [
    "# CODE WITH COMMENT ASSISTANCE - 20BCE1335,20BCE1345,20BCE1467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAD COUNT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323cc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_cascade =  cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "#eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#num_people = set()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    #eyes = eye_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    num_people = len(faces)\n",
    "    cv2.putText(frame, f'Heads: {num_people}', (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.75, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    #num_people.add(len(faces))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NO TRESPASSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        screenshot = frame.copy()\n",
    "        cv2.imshow('screenshot', screenshot)\n",
    "        engine.say(\"Alert: Trespassing Detected\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SPEECH RECOGNITION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from playsound import playsound\n",
    "\n",
    "# initialize speech recognition and text-to-speech engines\n",
    "r = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# define the trigger words and stop words\n",
    "trigger_words = [\"help\", \"save\", \"Run\", \"emergency\", \"disaster\", \"Bachao\"]\n",
    "stop_words = [\"stop sound\", \"shutdown\"]\n",
    "\n",
    "# define a function to handle recognized speech\n",
    "def handle_speech(phrase):\n",
    "    print(\"You said: \" + phrase)\n",
    "    if any(word in phrase for word in trigger_words):\n",
    "        print(\"Alert: Trigger word detected!\")\n",
    "        playsound(\"alert.wav\")  # play sound alert\n",
    "        engine.say(\"Alert: Trigger word detected!\")\n",
    "        engine.runAndWait()\n",
    "    elif any(word in phrase for word in stop_words):\n",
    "        print(\"Stop command detected!\")\n",
    "        playsound(None)  # stop sound alert\n",
    "\n",
    "# listen to the microphone and recognize speech\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Speak now...\")\n",
    "    while True:\n",
    "        try:\n",
    "            audio = r.listen(source)\n",
    "            phrase = r.recognize_google(audio)\n",
    "            handle_speech(phrase)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9408ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMBINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705406cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "import pyautogui\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize Pygame audio mixer\n",
    "import pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Define words to detect and their corresponding actions\n",
    "words_to_detect = {\n",
    "    \"help me\": \"help.mp3\",\n",
    "    \"save me\": \"save.mp3\",\n",
    "    \"run\": \"run.mp3\",\n",
    "    \"stop\": None,\n",
    "    \"shutdown\": None\n",
    "}\n",
    "\n",
    "# Define function to play audio\n",
    "def play_audio(filename):\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Define function to stop audio\n",
    "def stop_audio():\n",
    "    pygame.mixer.music.stop()\n",
    "\n",
    "# Initialize speech recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Initialize face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for head detection\n",
    "num_faces_last = 0\n",
    "t_start = time.time()\n",
    "\n",
    "# Define function to take a screenshot\n",
    "def take_screenshot():\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"screenshot_{timestamp}.png\"\n",
    "    pyautogui.screenshot(filename)\n",
    "    print(f\"Screenshot saved as {filename}\")\n",
    "\n",
    "# Loop to read frames from video capture\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in grayscale frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Count number of faces in current frame\n",
    "    num_faces = len(faces)\n",
    "\n",
    "    # If number of faces has changed, reset start time\n",
    "    if num_faces != num_faces_last:\n",
    "        t_start = time.time()\n",
    "\n",
    "    # If number of faces has not changed for 5 seconds, play alert\n",
    "    if num_faces_last != 0 and num_faces == 0 and time.time() - t_start > 5:\n",
    "        print(\"Alert: No faces detected\")\n",
    "        play_audio(\"alert.mp3\")\n",
    "\n",
    "    # Update number of faces in last frame\n",
    "    num_faces_last = num_faces\n",
    "\n",
    "    # Display number of faces on frame\n",
    "    cv2.putText(frame, f\"Faces: {num_faces}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Recognize speech\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        audio = r.listen(source)\n",
    "\n",
    "    try:\n",
    "        # Recognize speech using Google Speech Recognition API\n",
    "        text = r.recognize_google(audio)\n",
    "\n",
    "        # Print recognized speech\n",
    "        print(f\"You said: {text}\")\n",
    "\n",
    "        # Check if recognized speech contains words to detect\n",
    "        for word in words_to_detect:\n",
    "            if word in text.lower():\n",
    "                # If \"stop\" or \"shutdown\" is detected, stop audio\n",
    "                if word == \"stop\" or word == \"shutdown\":\n",
    "                    stop_audio()\n",
    "                    print(\"Audio stopped\")\n",
    "                # If \"save me\", \"help me\", or \"run\" is detected, take screenshot\n",
    "                elif word == \"save me\" or word == \"help me\" or word == \"run\":\n",
    "                    take_screenshot()\n",
    "                # Otherwise, play audio corresponding to word\n",
    "                    else:\n",
    "        \t\t\tfilename = words_to_detect[word]\n",
    "        \t\t\tif filename is not None:\n",
    "            \t\t\tplay_audio(filename)\n",
    "            \t\t\tprint(f\"{word} detected, playing audio: {filename}\")\n",
    "\n",
    "\texcept sr.UnknownValueError:\n",
    "    \t\t# Print an error message if speech is not recognized\n",
    "    \t\tprint(\"Speech recognition could not understand audio\")\n",
    "\n",
    "\texcept sr.RequestError as e:\n",
    "    \t\t# Print an error message if there is a problem with the speech recognition service\n",
    "    \t\tprint(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "\n",
    "\t# Show the video capture feed\n",
    "\tcv2.imshow(\"Video Capture\", frame)\n",
    "\n",
    "\t# Wait for key press to exit loop\n",
    "\tif cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    \t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENHANCED SPEECH REC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# initialize speech recognition and text-to-speech engines\n",
    "r = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# define the trigger words and stop words\n",
    "trigger_words = [\"help\", \"save\", \"run\", \"emergency\", \"disaster\"]\n",
    "stop_words = [\"stop sound\", \"shutdown\"]\n",
    "\n",
    "# define a function to handle recognized speech\n",
    "def handle_speech(phrase):\n",
    "    print(\"You said: \" + phrase)\n",
    "    if any(word in phrase for word in trigger_words):\n",
    "        print(\"Alert: Trigger word detected!\")\n",
    "        engine.say(\"Alert: Trigger word detected!\")\n",
    "        engine.runAndWait()\n",
    "        play_sound()\n",
    "    elif any(word in phrase for word in stop_words):\n",
    "        print(\"Stop command detected!\")\n",
    "        stop_sound()\n",
    "\n",
    "# define a function to play the alert sound\n",
    "def play_sound():\n",
    "    global sound_status\n",
    "    if not sound_status:\n",
    "        sound_status = True\n",
    "        os.system(\"afplay alert.wav &\")  # play sound alert in the background\n",
    "\n",
    "# define a function to stop the alert sound\n",
    "def stop_sound():\n",
    "    global sound_status\n",
    "    if sound_status:\n",
    "        sound_status = False\n",
    "        os.system(\"pkill afplay\")  # stop sound alert\n",
    "       \n",
    "# define a thread to listen to the microphone and recognize speech\n",
    "def listen_thread():\n",
    "    global sound_status\n",
    "    with sr.Microphone() as source:\n",
    "        r.adjust_for_ambient_noise(source)  # remove background noise\n",
    "        print(\"Speak now...\")\n",
    "        while True:\n",
    "            try:\n",
    "                audio = r.listen(source, phrase_time_limit=3)  # limit recognition time\n",
    "                phrase = r.recognize_google(audio)\n",
    "                handle_speech(phrase.lower())\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Could not request results; {0}\".format(e))\n",
    "            time.sleep(0.1)  # add a small delay to avoid high CPU usage\n",
    "\n",
    "# initialize the sound status\n",
    "sound_status = False\n",
    "\n",
    "# start the listen thread\n",
    "listen_thread = threading.Thread(target=listen_thread)\n",
    "listen_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENHANCED HEAD COUNT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained face detector (HOG+SVM)\n",
    "face_detector = cv2.HOGDescriptor()\n",
    "face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Open the camera device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the camera resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Define a threshold for face detection confidence\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Initialize variables for face tracking\n",
    "prev_faces = []\n",
    "face_tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for faster processing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using the pre-trained face detector\n",
    "    faces, confidences = face_detector.detectMultiScale(gray, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Filter out low-confidence face detections\n",
    "    high_confidence_faces = [face for face, confidence in zip(faces, confidences) if confidence > CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    # Track the faces using a Kalman filter\n",
    "    if prev_faces:\n",
    "        ok, bbox = face_tracker.update(frame)\n",
    "        if ok:\n",
    "            high_confidence_faces.append(bbox)\n",
    "        else:\n",
    "            prev_faces = []\n",
    "\n",
    "    # Draw a rectangle around each detected face\n",
    "    for (x, y, w, h) in high_confidence_faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Update the face tracker\n",
    "    if high_confidence_faces:\n",
    "        face_tracker.init(frame, high_confidence_faces[0])\n",
    "\n",
    "    # Display the frame with the detected faces\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2253e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENHANCED NO TRESSPASSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load the face and eye cascade classifiers\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# Initialize variables for face tracking\n",
    "face_detected = False\n",
    "face_start_time = 0\n",
    "face_end_time = 0\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw a rectangle around each face and detect eyes\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "        # Keep track of face detection time\n",
    "        if not face_detected:\n",
    "            face_detected = True\n",
    "            face_start_time = time.time()\n",
    "    else:\n",
    "        # Keep track of face disappearance time\n",
    "        if face_detected:\n",
    "            face_detected = False\n",
    "            face_end_time = time.time()\n",
    "\n",
    "            # If face was visible for more than 3 seconds, take a screenshot\n",
    "            if face_end_time - face_start_time > 3:\n",
    "                screenshot = frame.copy()\n",
    "                cv2.imshow('screenshot', screenshot)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # Write the frame to the video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and video writer\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PROFILER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import pydotplus\n",
    "import cv2\n",
    "\n",
    "# Define the function to be profiled\n",
    "def main():\n",
    "    # Paste the code in the main function to conduct profiling\n",
    "\n",
    "# Use cProfile to profile the function\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "main()\n",
    "pr.disable()\n",
    "\n",
    "# Store the profile data in a string buffer\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats()\n",
    "\n",
    "# Generate the graph using pydotplus\n",
    "dot = pydotplus.graph_from_dot_data(s.getvalue())\n",
    "if dot is not None:\n",
    "    dot.write_png('profile.png')\n",
    "else:\n",
    "    print(\"Unable to generate the profile graph.\")\n",
    "\n",
    "dot_data = s.getvalue()\n",
    "if 'digraph' in dot_data or 'graph' in dot_data:\n",
    "    dot = pydotplus.graph_from_dot_data(dot_data)\n",
    "    dot.write_png('profile.png')\n",
    "else:\n",
    "    print(\"Unable to generate the profile graph.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a953a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CYCLOMATIC COMPLEXITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99199e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import radon\n",
    "from radon.complexity import cc_visit\n",
    "\n",
    "with open('headcountTarp.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "cc = radon.complexity.cc_visit(code)\n",
    "\n",
    "values = [complexity for _, complexity, _, _ in cc]\n",
    "plt.plot(values)\n",
    "plt.title('Cyclomatic Complexity')\n",
    "plt.xlabel('Function')\n",
    "plt.ylabel('Cyclomatic Complexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEAT MAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained face detector (HOG+SVM)\n",
    "face_detector = cv2.HOGDescriptor()\n",
    "face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Open the camera device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the camera resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Initialize variables for face tracking\n",
    "prev_faces = []\n",
    "face_tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "# Define the heatmap grid\n",
    "heatmap_grid = np.zeros((480//20, 640//20))\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for faster processing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using the pre-trained face detector\n",
    "    faces, confidences = face_detector.detectMultiScale(gray, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Filter out low-confidence face detections\n",
    "    high_confidence_faces = [face for face, confidence in zip(faces, confidences) if confidence > 0.5]\n",
    "\n",
    "    # Track the faces using a Kalman filter\n",
    "    if prev_faces:\n",
    "        ok, bbox = face_tracker.update(frame)\n",
    "        if ok:\n",
    "            high_confidence_faces.append(bbox)\n",
    "        else:\n",
    "            prev_faces = []\n",
    "\n",
    "    # Update the face tracker\n",
    "    if high_confidence_faces:\n",
    "        face_tracker.init(frame, high_confidence_faces[0])\n",
    "\n",
    "    # Update the heatmap grid\n",
    "    for (x, y, w, h) in high_confidence_faces:\n",
    "        heatmap_grid[y//20:(y+h)//20, x//20:(x+w)//20] += 1\n",
    "\n",
    "    # Display the frame with the detected faces\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Create the heatmap plot\n",
    "sns.heatmap(heatmap_grid, cmap='coolwarm', xticklabels=False, yticklabels=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SCATTER MAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained face detector (HOG+SVM)\n",
    "face_detector = cv2.HOGDescriptor()\n",
    "face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Open the camera device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the camera resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Define a threshold for face detection confidence\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Initialize variables for face tracking\n",
    "prev_faces = []\n",
    "face_tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "# Initialize list for face positions\n",
    "face_positions = []\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for faster processing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using the pre-trained face detector\n",
    "    faces, confidences = face_detector.detectMultiScale(gray, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Filter out low-confidence face detections\n",
    "    high_confidence_faces = [face for face, confidence in zip(faces, confidences) if confidence > CONFIDENCE_THRESHOLD]\n",
    "\n",
    "    # Track the faces using a Kalman filter\n",
    "    if prev_faces:\n",
    "        ok, bbox = face_tracker.update(frame)\n",
    "        if ok:\n",
    "            high_confidence_faces.append(bbox)\n",
    "        else:\n",
    "            prev_faces = []\n",
    "\n",
    "    # Draw a rectangle around each detected face and add its position to the list\n",
    "    for (x, y, w, h) in high_confidence_faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        face_positions.append((x + w/2, y + h/2))\n",
    "\n",
    "    # Update the face tracker\n",
    "    if high_confidence_faces:\n",
    "        face_tracker.init(frame, high_confidence_faces[0])\n",
    "\n",
    "    # Display the frame with the detected faces\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Create a scatter plot of the face positions\n",
    "x, y = zip(*face_positions)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Scatter Plot of Detected Faces\")\n",
    "plt.xlabel(\"X position\")\n",
    "plt.ylabel(\"Y position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de794c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BAR MAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from playsound import playsound\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialize speech recognition and text-to-speech engines\n",
    "r = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# define the trigger words and stop words\n",
    "trigger_words = [\"help\", \"save\", \"Run\", \"disaster\"]\n",
    "stop_words = [\"stop sound\", \"shutdown\"]\n",
    "\n",
    "# define a stack to keep track of recognized phrases\n",
    "recognized_phrases = []\n",
    "\n",
    "# define a function to handle recognized speech\n",
    "def handle_speech(phrase):\n",
    "    print(\"You said: \" + phrase)\n",
    "    recognized_phrases.append(phrase)  # add phrase to stack\n",
    "    if any(word in phrase for word in trigger_words):\n",
    "        print(\"Alert: Trigger word detected!\")\n",
    "        playsound(\"alert.wav\")  # play sound alert\n",
    "        engine.say(\"Alert: Trigger word detected!\")\n",
    "        engine.runAndWait()\n",
    "    elif any(word in phrase for word in stop_words):\n",
    "        print(\"Stop command detected!\")\n",
    "        playsound(None)  # stop sound alert\n",
    "\n",
    "# initialize the plot\n",
    "trigger_word_counts = [0 for _ in trigger_words]\n",
    "trigger_word_counts.append(0)  # initialize non-trigger words count\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = plt.bar(trigger_words + [\"Non-Trigger Words\"], trigger_word_counts)\n",
    "plt.xlabel(\"Trigger Words\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.title(\"Occurrences of Trigger Words and Non-Trigger Words\")\n",
    "plt.ylim(bottom=10)  # Set y-axis plot scale to at least 10\n",
    "plt.show()\n",
    "\n",
    "# listen to the microphone and recognize speech\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Speak now...\")\n",
    "    while True:\n",
    "        try:\n",
    "            audio = r.listen(source)\n",
    "            phrase = r.recognize_google(audio)\n",
    "            handle_speech(phrase)\n",
    "\n",
    "            # update the plot with the new counts\n",
    "            for i, trigger_word in enumerate(trigger_words):\n",
    "                if trigger_word in phrase.lower():\n",
    "                    trigger_word_counts[i] += 1\n",
    "            non_trigger_word_count = sum([1 for word in recognized_phrases if all(trigger_word not in word.lower() for trigger_word in trigger_words)])\n",
    "            trigger_word_counts[-1] = non_trigger_word_count\n",
    "            plt.clf()  # clear the plot\n",
    "            bar_plot = plt.bar(trigger_words + [\"Non-Trigger Words\"], trigger_word_counts,width=1)\n",
    "            plt.xlabel(\"Trigger Words\")\n",
    "            plt.ylabel(\"Occurrences\")\n",
    "            plt.title(\"Occurrences of Trigger Words and Non-Trigger Words\")\n",
    "            plt.ylim(bottom=0)  # Set y-axis plot scale to at least 10\n",
    "            plt.pause(0.01)  # pause to allow the plot to update\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results; {0}\".format(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
